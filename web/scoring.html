<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>LegacyCodeBench | Scoring</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <link rel="stylesheet" href="assets/styles.css">
</head>

<body>
  <header>
    <div class="container"
      style="display:flex;justify-content:space-between;align-items:center;padding:1.25rem 1.25rem 1rem;">
      <div class="logo">LegacyCodeBench</div>
      <nav aria-label="Primary">
        <a href="index.html">Overview</a>
        <a href="leaderboard.html">Leaderboard</a>
        <a href="datasets.html">Datasets</a>
        <a href="scoring.html" class="active">Scoring</a>
        <a href="docs.html">Docs</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="hero" aria-labelledby="scoring-title">
      <h1 id="scoring-title">Scoring Methodology v2.3.1</h1>
      <p>LegacyCodeBench v2.3.1 uses a deterministic, ground-truth based evaluation. AI-generated documentation is
        validated
        against source code elements extracted via static analysis.</p>
      <p class="table-hint">All scores are deterministic and reproducible. No LLM-as-judge required.</p>
    </section>

    <section aria-labelledby="lcb-score">
      <p class="page-description">
        LegacyCodeBench uses a 3-pillar scoring system (v2.3.1) to ensure generated documentation is both legible and
        executable.
      </p>
      <div class="formula">
        LCB Score = (0.30 &times; SC) + (0.45 &times; DQ) + (0.25 &times; BF)
      </div>
      <p class="table-hint">Scores range from 0-100. Higher is better.</p>
    </section>

    <section aria-labelledby="components">
      <h2 class="section-title" id="components">Evaluation Components</h2>
      <div class="grid grid-2">
        <div class="card">
          <h3>Structural Completeness (SC) — 30%</h3>
          <p>Element coverage vs auto-extracted ground truth from source code.</p>
          <ul class="list-muted">
            <li>Data structures documented</li>
            <li>Paragraphs/sections covered</li>
            <li>Business rules captured</li>
            <li>File dependencies mentioned</li>
          </ul>
          <p class="table-hint">Ground truth extracted via static analysis (95%+ automated).</p>
        </div>
        <div class="card">
          <h3>Documentation Quality (DQ) - 45%</h3>
          <p>Measures the clarity, structure, and abstraction level of the documentation.</p>
          <ul class="check-list">
            <li><strong>Readability</strong> - Flesch-Kincaid & structure analysis</li>
            <li><strong>Traceability</strong> - Accuracy of code references</li>
            <li><strong>Abstraction</strong> - High-level vs low-level balance</li>
          </ul>
          <p class="table-hint">Fully deterministic - no LLM involved.</p>
        </div>

        <div class="card">
          <h3>Behavioral Fidelity (BF) - 25%</h3>
          <p>Verifies that the documented logic can be used to regenerate the original code's behavior.</p>
          <ul class="check-list">
            <li><strong>Claim Verification</strong> - Testing claims against code</li>
            <li><strong>Execution Testing</strong> - Running compiled code</li>
            <li><strong>Pattern Matching</strong> - Validating external calls (BSM)</li>
          </ul>
          <p class="table-hint">Requires Docker (heuristic fallback available).</p>
        </div>--enable-execution</code> for full BF testing.</p>
      </div>
      </div>
    </section>

    <section aria-labelledby="tiers">
      <h2 class="section-title" id="tiers">Task Complexity Tiers</h2>
      <p>Tasks are organized by COBOL program complexity:</p>
      <table>
        <thead>
          <tr>
            <th>Tier</th>
            <th>Name</th>
            <th>LOC Range</th>
            <th>Characteristics</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>T1</td>
            <td>Basic</td>
            <td>300-500</td>
            <td>Simple, linear programs with minimal branching</td>
          </tr>
          <tr>
            <td>T2</td>
            <td>Moderate</td>
            <td>500-1000</td>
            <td>PERFORM loops, file I/O operations</td>
          </tr>
          <tr>
            <td>T3</td>
            <td>Complex</td>
            <td>1000-2000</td>
            <td>External calls, complex business rules</td>
          </tr>
          <tr>
            <td>T4</td>
            <td>Enterprise</td>
            <td>2000+</td>
            <td>GO TO spaghetti, CICS/DB2 integration</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section aria-labelledby="ground-truth">
      <h2 class="section-title" id="ground-truth">Ground Truth Generation</h2>
      <p>Ground truth is automatically extracted from COBOL source code using static analysis:</p>
      <ul class="list-muted">
        <li><strong>Data Structures</strong> — WORKING-STORAGE, FILE SECTION, LINKAGE SECTION parsing</li>
        <li><strong>Control Flow</strong> — PERFORM, GO TO, paragraph/section analysis</li>
        <li><strong>Business Rules</strong> — IF/EVALUATE conditions, calculations, validations</li>
        <li><strong>Dependencies</strong> — CALL statements, COPY statements, file assignments</li>
        <li><strong>Error Handlers</strong> — ON ERROR, AT END, INVALID KEY patterns</li>
      </ul>
      <div class="callout">
        Ground truth is 95%+ automated. Only edge cases require human review.
      </div>
    </section>

    <section aria-labelledby="examples">
      <h2 class="section-title" id="examples">Scoring Example</h2>
      <div class="card">
        <h3>Sample Evaluation</h3>
        <div class="code-block">
          Component Scores:
          Structural Completeness (SC): 80%
          Documentation Quality (DQ): 70%
          Behavioral Fidelity (BF): 85%

          LCB Score = (0.30 &times; 80) + (0.45 &times; 70) + (0.25 &times; 85)
          = 24 + 14 + 42.5
          = 80.5
        </div>
        <p><strong>Result: LCB Score 80.5/100</strong></p>
      </div>
    </section>

    <section aria-labelledby="faq">
      <h2 class="section-title" id="faq">FAQ</h2>
      <details class="faq">
        <summary>How is ground truth generated?</summary>
        <p>Ground truth is extracted automatically from COBOL source code using static analysis. The benchmark parses
          DATA DIVISION, PROCEDURE DIVISION, and analyzes control flow, business rules, and dependencies. This is 95%+
          automated with high confidence scores.</p>
      </details>
      <details class="faq">
        <summary>What if behavioral fidelity testing is unavailable?</summary>
        <p>BF testing requires Docker with GnuCOBOL. If unavailable, a placeholder score is used. Run with
          <code>--enable-execution</code> for full BF evaluation.
        </p>
      </details>
      <details class="faq">
        <summary>How does LLM-as-judge work?</summary>
        <p>A separate LLM (different from the model being evaluated) assesses documentation quality. This prevents
          self-evaluation bias. The judge model can be configured with <code>--judge-model</code>.</p>
      </details>
      <details class="faq">
        <summary>Can I reproduce the scores?</summary>
        <p>Yes. All evaluation is deterministic. Run
          <code>python -m legacycodebench evaluate --task-id &lt;id&gt; --submission &lt;path&gt;</code> to get
          identical scores.
        </p>
      </details>
    </section>
  </main>

  <footer>
    <div class="container" style="display:flex;flex-wrap:wrap;gap:1rem;justify-content:space-between;">
      <span>LegacyCodeBench &middot; Kalmantic AI Labs + Hexaview Tech</span>
      <span>GitHub &middot; <a href="https://github.com/kalmantic/legacycodebench" target="_blank"
          rel="noreferrer">kalmantic/legacycodebench</a></span>
      <span>Contact &middot; <a href="mailto:nikita@kalmantic.com">nikita@kalmantic.com</a></span>
    </div>
  </footer>
</body>

</html>