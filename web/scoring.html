<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>LegacyCodeBench | Scoring</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <link rel="stylesheet" href="assets/styles.css">
</head>
<body>
  <header>
    <div class="container" style="display:flex;justify-content:space-between;align-items:center;padding:1.25rem 1.25rem 1rem;">
      <div class="logo">LegacyCodeBench</div>
      <nav aria-label="Primary">
        <a href="index.html">Overview</a>
        <a href="leaderboard.html">Leaderboard</a>
        <a href="datasets.html">Datasets</a>
        <a href="scoring.html" class="active">Scoring</a>
        <a href="docs.html">Docs</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="hero" aria-labelledby="scoring-title">
      <h1 id="scoring-title">Scoring Methodology</h1>
      <p>LegacyCodeBench mirrors SWE-bench’s rigor while focusing on documentation-first evaluation. Scores are deterministic and reproducible inside Docker.</p>
    </section>

    <section aria-labelledby="task-types">
      <h2 class="section-title" id="task-types">Task Types</h2>
      <div class="grid grid-3">
        <div class="card">
          <h3>Documentation</h3>
          <p>Systems must generate Markdown documentation covering purpose, business rules, edge cases, and data structures.</p>
        </div>
        <div class="card">
          <h3>Understanding</h3>
          <p>Systems must output JSON capturing dependencies, business rules, and data flow from COBOL programs.</p>
        </div>
        <div class="card">
          <h3>Hybrid Evaluation</h3>
          <p>50% documentation score + 50% understanding score → overall benchmark score.</p>
        </div>
      </div>
    </section>

    <section aria-labelledby="documentation-scoring">
      <h2 class="section-title" id="documentation-scoring">Documentation Scoring</h2>
      <p>Each documentation submission is evaluated using weighted components:</p>
      <ul class="list-muted">
        <li>Required sections present (40%).</li>
        <li>Business rule coverage heuristics (30%).</li>
        <li>Markdown format quality (20%).</li>
        <li>Appropriate length (10%).</li>
      </ul>
      <div class="code-block">
doc_score = 0.40 × sections + 0.30 × rules + 0.20 × format + 0.10 × length
      </div>
      <p class="table-hint">Reference documentation from COBOL experts is used to validate automated scores.</p>
    </section>

    <section aria-labelledby="understanding-scoring">
      <h2 class="section-title" id="understanding-scoring">Understanding Scoring</h2>
      <p>Understanding tasks are scored via F1 based on ground-truth graphs extracted from COBOL:</p>
      <div class="code-block">
precision = true_positives / predicted<br>
recall = true_positives / actual<br>
f1 = 2 × (precision × recall) / (precision + recall)
      </div>
      <p>Separate F1 scores are computed for dependencies, business rules, and data flow, then averaged.</p>
    </section>

    <section aria-labelledby="overall-score">
      <h2 class="section-title" id="overall-score">Overall Score</h2>
      <div class="code-block">
overall = 0.50 × documentation + 0.50 × understanding
      </div>
      <p>Scores are capped between 0 and 100. Performance tiers follow PRD definitions (>60% useful, 40–60% research baseline, etc.).</p>
    </section>

    <section aria-labelledby="examples">
      <h2 class="section-title" id="examples">Examples</h2>
      <div class="card">
        <h3>Documentation Example</h3>
        <p>Sections: 1.0 · Rules: 0.7 · Format: 1.0 · Length: 1.0 → doc_score = 0.87 (87%).</p>
      </div>
      <div class="card">
        <h3>Understanding Example</h3>
        <p>Dependencies F1: 0.72, Business Rules F1: 0.60, Data Flow F1: 0.85 → understanding = 0.72.</p>
      </div>
    </section>

    <section aria-labelledby="faq">
      <h2 class="section-title" id="faq">FAQ</h2>
      <details class="faq">
        <summary>How often are scores reviewed?</summary>
        <p>Every submission is re-run inside Docker. Maintainers spot-check reference documentation for drift.</p>
      </details>
      <details class="faq">
        <summary>Are human validators involved?</summary>
        <p>Yes. 5% of documentation outputs are checked manually using the “Would you use this?” criterion.</p>
      </details>
      <details class="faq">
        <summary>Can I reproduce the scores?</summary>
        <p>Yes. All evaluation scripts are open source and run via <code>legacycodebench evaluate</code>.</p>
      </details>
    </section>
  </main>

  <footer>
    <div class="container" style="display:flex;flex-wrap:wrap;gap:1rem;justify-content:space-between;">
      <span>© 2025 LegacyCodeBench · Kalmantic Applied AI Lab</span>
      <span>GitHub · <a href="https://github.com/kalmanticlegacycodebench" target="_blank" rel="noreferrer">kalmantic/legacycodebench</a></span>
      <span>Contact · <a href="mailto:team@kalmantic.ai">team@kalmantic.ai</a></span>
    </div>
  </footer>
</body>
</html>

